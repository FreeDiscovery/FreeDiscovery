{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\nData Ingestion Example\n----------------------\n\nAn example illustrating the data ingestion in FreeDiscovery\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from __future__ import print_function\n\nimport requests\nimport pandas as pd\nimport json\nimport os.path\n\npd.options.display.float_format = '{:,.3f}'.format\npd.options.display.expand_frame_repr = False\n\ndataset_name = \"treclegal09_2k_subset\"     # see list of available datasets\n\nBASE_URL = \"http://localhost:5001/api/v0\"  # FreeDiscovery server URL\n\nif __name__ == '__main__':\n\n    print(\" 0. Load the test dataset\")\n    url = BASE_URL + '/example-dataset/{}'.format(dataset_name)\n    print(\" GET\", url)\n    input_ds = requests.get(url).json()\n\n    # To use a custom dataset, simply specify the following variables\n    # create a custom dataset definition for ingestion\n    data_dir = input_ds['metadata']['data_dir']\n    dataset_definition = [{'document_id': row['document_id'],\n                           'file_path': os.path.join(data_dir, row['file_path'])} \\\n                                   for row in input_ds['dataset']]\n\n\n    # 2. Ingest a dataset specified by a path to each file in the dataset\n    print(\"\\n1.a Load dataset and initalize feature extraction\")\n    url = BASE_URL + '/feature-extraction'\n    print(\" POST\", url)\n    res = requests.post(url, json={'use_hashing': True}).json()\n\n    dsid = res['id']\n    print(\"   => received {}\".format(list(res.keys())))\n    print(\"   => dsid = {}\".format(dsid))\n\n    print(\"\\n1.b Start feature extraction\")\n\n    url = BASE_URL+'/feature-extraction/{}'.format(dsid)\n    print(\" POST\", url)\n    res = requests.post(url, json={'dataset_definition': dataset_definition})\n\n\n    print(\"\\n2.d. check the parameters of the extracted features\")\n    url = BASE_URL + '/feature-extraction/{}'.format(dsid)\n    print(' GET', url)\n    res = requests.get(url).json()\n\n    print('\\n'.join(['     - {}: {}'.format(key, val)\n          for key, val in res.items() if \"filenames\" not in key]))\n\n    print(\"\\n3. Examine the id mapping\\n\")\n\n    method = BASE_URL + \"/feature-extraction/{}/id-mapping\".format(dsid)\n    print('\\n GET', method)\n    data = {'data': [{'internal_id': row['internal_id']} for row in input_ds['dataset'][:3]]}\n    print('   DATA', json.dumps(data))\n    res = requests.post(method, json=data).json()\n\n    print(' Response:')\n    print('  ', json.dumps(res, indent=4))\n\n    # 4. Cleaning\n    print(\"\\n5.a Delete the extracted features\")\n    url = BASE_URL + '/feature-extraction/{}'.format(dsid)\n    print(\" DELETE\", url)\n    requests.delete(url)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}